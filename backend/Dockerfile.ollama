# Multi-stage build for Ollama + Phi-3 Mini
FROM ollama/ollama:latest as ollama-base

# Install curl for health checks
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Create ollama user and directories
RUN useradd -r -s /bin/false -m -d /usr/share/ollama ollama
RUN mkdir -p /usr/share/ollama/.ollama && chown ollama:ollama /usr/share/ollama/.ollama

# Start ollama service and pull phi3:mini
RUN ollama serve & sleep 10 && ollama pull phi3:mini

# Production stage
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Copy pre-downloaded model from ollama-base stage
COPY --from=ollama-base /usr/share/ollama/.ollama /root/.ollama

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install additional packages for Ollama integration
RUN pip install --no-cache-dir \
    ollama \
    requests \
    asyncio

# Copy application code
COPY . .

# Create startup script
RUN echo '#!/bin/bash\n\
ollama serve &\n\
sleep 10\n\
python app.py' > start.sh && chmod +x start.sh

# Expose ports
EXPOSE 8000 11434

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:11434/api/tags || exit 1

# Start both Ollama and the Flask app
CMD ["./start.sh"]